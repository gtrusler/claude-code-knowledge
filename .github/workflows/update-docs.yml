name: Update Claude Code Docs

on:
  schedule:
    # Run daily at 6 AM UTC
    - cron: '0 6 * * *'
  workflow_dispatch: # Allow manual trigger

jobs:
  update-docs:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install requests beautifulsoup4 markdownify
    
    - name: Create scraper script
      run: |
        cat > scraper.py << 'SCRIPT'
        import requests
        from bs4 import BeautifulSoup
        import markdownify
        from datetime import datetime
        import sys
        
        def scrape_claude_docs():
            """Scrape Claude Code documentation from Anthropic"""
            base_url = "https://docs.anthropic.com/en/docs/claude-code"
            
            try:
                # Main Claude Code page
                response = requests.get(base_url, timeout=30)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Look for main content area
                content = soup.find('main') or soup.find('article') or soup.find('div', {'class': 'content'})
                
                if not content:
                    # Try to get any reasonable content
                    content = soup.find('body')
                
                if not content:
                    return "Unable to locate content. Page structure may have changed."
                
                # Convert to markdown
                markdown = markdownify.markdownify(str(content), heading_style="ATX")
                
                # Clean up
                lines = markdown.split('\n')
                cleaned_lines = []
                for line in lines:
                    # Skip empty lines at the start
                    if not cleaned_lines and not line.strip():
                        continue
                    cleaned_lines.append(line)
                
                markdown = '\n'.join(cleaned_lines)
                
                # Add metadata
                header = f"""# Claude Code Official Documentation

*Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M UTC')}*
*Source: {base_url}*

---

"""
                return header + markdown
                
            except requests.RequestException as e:
                return f"Error fetching documentation: {str(e)}"
            except Exception as e:
                return f"Error processing documentation: {str(e)}"
        
        if __name__ == "__main__":
            docs_content = scrape_claude_docs()
            
            with open('official/docs.md', 'w', encoding='utf-8') as f:
                f.write(docs_content)
            
            print("Documentation updated successfully")
        SCRIPT
    
    - name: Run scraper
      run: python scraper.py
    
    - name: Create subpage scraper
      run: |
        cat > scrape_subpages.py << 'SCRIPT'
        import requests
        from bs4 import BeautifulSoup
        import markdownify
        import os
        import time
        
        # List of subpages to scrape
        subpages = [
            'getting-started',
            'slash-commands',
            'mcp',
            'hooks',
            'settings'
        ]
        
        for page in subpages:
            url = f"https://docs.anthropic.com/en/docs/claude-code/{page}"
            
            try:
                # Be nice to the server
                time.sleep(1)
                
                response = requests.get(url, timeout=30)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    content = soup.find('main') or soup.find('article') or soup.find('div', {'class': 'content'})
                    
                    if content:
                        markdown = markdownify.markdownify(str(content), heading_style="ATX")
                        
                        # Append to main docs
                        with open('official/docs.md', 'a', encoding='utf-8') as f:
                            f.write(f"\n\n---\n\n## {page.replace('-', ' ').title()}\n\n")
                            f.write(markdown)
                        
                        print(f"Added {page} to documentation")
                    else:
                        print(f"No content found for {page}")
                else:
                    print(f"Failed to fetch {page}: HTTP {response.status_code}")
            except Exception as e:
                print(f"Error processing {page}: {e}")
        
        print("Subpage scraping complete")
        SCRIPT
    
    - name: Run subpage scraper
      run: python scrape_subpages.py
      continue-on-error: true
    
    - name: Check for changes
      id: verify
      run: |
        if [[ -n $(git status --porcelain) ]]; then
          echo "changes=true" >> $GITHUB_OUTPUT
          echo "Changes detected in documentation"
        else
          echo "changes=false" >> $GITHUB_OUTPUT
          echo "No changes detected"
        fi
    
    - name: Commit and push
      if: steps.verify.outputs.changes == 'true'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add official/docs.md
        git commit -m "Update Claude Code documentation - $(date +'%Y-%m-%d')"
        git push
